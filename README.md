# My Deep Learning Reading List
A list of papers that I believe are either important or useful for understanding deep learning.

# Start Here
*The most landmark and important papers in deep learning. Read these papers in order.*

| Paper                                   | Description |
|----------------------------------------|---------------------------------------|
| [Backpropagation](https://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) | The original paper that described the backpropagation algorithm, the central algorithm behind how modern deep learning models work. |
| [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | Often considered the paper that kickstarted the modern era of deep learning, this paper proposed the idea of just stacking a bunch of layers as a performance improvement method |
| [The Adam Optimizer](https://arxiv.org/pdf/1412.6980) | A key landmark in optimization algorithms for deep learning models, this paper proposes a new framework for weight updates during backpropagation. |
| [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) | A huge breakthrough in sequential understanding for deep learning models, allowing them to store and tune the information they saw previously and use it for future predictions. |
| [Attention is All You Need](https://arxiv.org/pdf/1706.03762) | Arugably one of the two most important papers in modern deep learning, along with AlexNet, this paper proposed the Transformer, the building block to large language models, and a huge milestone in language understanding for deep learning models. |
| [Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602) | A key breakthrough in reinforcement learning, this paper combined modern efforts in deep learning with goal-based learning approaches, instead of loss-based approaches. |
| [Denoising Diffusion Models](https://arxiv.org/pdf/2006.11239) | This paper proposed an architecture and algorithm for image generation that produced highly life-like images, a key landmark in artificial image understanding. |
| [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165) | This is the paper that was released alongside the original ChatGPT, explaining how very large language models could demonstrate viable performance in tasks they had limited knowledge in. |

## Computer Vision
*Important works in computer vision.*

| Paper                                   |
|----------------------------------------|
| [Residual Networks](https://arxiv.org/pdf/1512.03385) |

## Traditional NLP
*Papers about pre-transformer NLP breakthroughs.*

| Paper                                   |
|----------------------------------------|
| [word2vec](https://arxiv.org/pdf/1301.3781) |
| [Nucleus Sampling](https://arxiv.org/pdf/1904.09751) |

## Transformers
*Papers about transformers and their applications.*

| Paper                                   |
|----------------------------------------|
| [BERT](https://arxiv.org/pdf/1810.04805) |
| [Vision Transformers](https://arxiv.org/pdf/2010.11929) |

## Large Language Model Hype Train
*Papers about large language models and related works on them.*

| Paper                                   |
|----------------------------------------|
| [Chain of Thought Reasoning](https://arxiv.org/pdf/2201.11903) |
| [Instruction Tuning](https://arxiv.org/pdf/2203.02155) |
| [Speculative Decoding](https://arxiv.org/pdf/2302.01318) |

## Historically Important
*Historically famous papers that are still used today, but not essential reading.*

| Paper                                   |
|----------------------------------------|
| [ReLU](https://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf) |
| [UNet](https://arxiv.org/pdf/1505.04597) |
| [XGBoost](https://arxiv.org/pdf/1603.02754) |
| [Batch Normalization](https://arxiv.org/pdf/1502.03167) |

## Generative Models
*Papers about deep generative models.*

| Paper                                   |
|----------------------------------------|
| [Variational Autoencoders](https://arxiv.org/pdf/1312.6114) |
| [GANs](https://arxiv.org/pdf/1406.2661) |

## Reinforcement Learning
*Papers about reinforcement learning*

| Paper                                   |
|----------------------------------------|
| [Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347) |

## Deep Learning Science
*Papers about research about deep learning.*

| Paper                                   |
|----------------------------------------|
| [Contrastive Representation Learning (CLIP)](https://arxiv.org/pdf/2203.02053) |
| [The Lottery Ticket Hypothesis](https://arxiv.org/pdf/1803.03635) |

## Applied Deep Learning
*Papers using deep learning to solve huge problems.*

| Paper                                   |
|----------------------------------------|
| [AlphaFold](https://www.nature.com/articles/s41586-021-03819-2) |

## DL Theory
*Papers exploring the math of deep learning advancements.*

| Paper                                   |
|----------------------------------------|
| [Dropout](https://papers.nips.cc/paper/2013/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf) |
| [Low Rank Adaptation](https://arxiv.org/pdf/2106.09685) |
| [GANs as a Nash Equilibrium](https://arxiv.org/pdf/1706.08500v6) |
